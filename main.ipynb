{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ombx8ieji4Ze"
   },
   "source": [
    "# Code by TCE team at Qur'an QA 2023 shared task A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftqSfDklodvR"
   },
   "source": [
    "# Installation\n",
    "\n",
    "I use [rclone](https://rclone.org/) to access my drive without asking for permission everytime.\n",
    "The code accesses a file called colab4 which has my drive access token, you may replicate this on your side or just ignore this altogether and download files manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "gDxbUn4SqkZn",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  4734  100  4734    0     0  13787      0 --:--:-- --:--:-- --:--:-- 14005\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://rclone.org/install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!chmod +x install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!sudo bash install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PADO5nbTdSM1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "e4_yhMkDusbM",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rclone' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rclone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "033ZSN1jEGNB"
   },
   "source": [
    "## Clone repo and prepare the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t2k2ZNV3dVPz",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'quran-qa'...\n"
     ]
    }
   ],
   "source": [
    "repo_url = f\"https://github.com/Mohamedbasem1/Graduation-Project.git\"\n",
    "!git clone $repo_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\medob\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (3.2.0)\n",
      "Collecting sentence-transformers (from -r requirements.txt (line 2))\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers (from -r requirements.txt (line 3))\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 437.8 kB/s eta 0:00:00\n",
      "Collecting farasapy==0.0.14 (from -r requirements.txt (line 4))\n",
      "  Downloading farasapy-0.0.14-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting fuzzysearch==0.7.3 (from -r requirements.txt (line 5))\n",
      "  Downloading fuzzysearch-0.7.3.tar.gz (112 kB)\n",
      "     ---------------------------------------- 0.0/112.7 kB ? eta -:--:--\n",
      "     -------------- ------------------------ 41.0/112.7 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 112.7/112.7 kB 1.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting gdown==4.4.0 (from -r requirements.txt (line 6))\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting xmltodict==0.13.0 (from -r requirements.txt (line 7))\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting PyArabic==0.6.15 (from -r requirements.txt (line 8))\n",
      "  Downloading PyArabic-0.6.15-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ujson==5.7.0 (from -r requirements.txt (line 9))\n",
      "  Downloading ujson-5.7.0.tar.gz (7.2 MB)\n",
      "     ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/7.2 MB 3.6 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.3/7.2 MB 3.5 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.5/7.2 MB 3.7 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.6/7.2 MB 3.7 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.7/7.2 MB 3.7 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.7/7.2 MB 2.7 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.9/7.2 MB 2.9 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 1.4/7.2 MB 3.9 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 1.6/7.2 MB 3.8 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 1.6/7.2 MB 3.8 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 1.6/7.2 MB 3.8 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.7/7.2 MB 3.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.0/7.2 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.0/7.2 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.0/7.2 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 2.2/7.2 MB 3.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 2.5/7.2 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 3.0/7.2 MB 3.6 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 3.4/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 3.5/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 3.7/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 3.9/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.0/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.2/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 4.4/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 4.5/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 4.7/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 4.9/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.1/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 5.3/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 5.4/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 5.6/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 5.8/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 6.0/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 6.1/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.3/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 6.5/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 6.6/7.2 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 6.8/7.2 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 7.0/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.1/7.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.2/7.2 MB 3.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting evaluate (from -r requirements.txt (line 10))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting disjoint-set (from -r requirements.txt (line 11))\n",
      "  Downloading disjoint_set-0.8.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting tabulate (from -r requirements.txt (line 12))\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting certifi==2021.10.8 (from -r requirements.txt (line 13))\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer==2.0.11 (from -r requirements.txt (line 14))\n",
      "  Downloading charset_normalizer-2.0.11-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting idna==3.3 (from -r requirements.txt (line 15))\n",
      "  Downloading idna-3.3-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting snowballstemmer==2.0.0 (from -r requirements.txt (line 16))\n",
      "  Downloading snowballstemmer-2.0.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting Arabic-Stopwords==0.3 (from -r requirements.txt (line 17))\n",
      "  Downloading Arabic_Stopwords-0.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pytrec-eval==0.5 (from -r requirements.txt (line 18))\n",
      "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting accelerate (from -r requirements.txt (line 19))\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for faiss-gpu\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-terrier==0.7.1 in c:\\users\\medob\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install --no-deps python-terrier==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pwd (from versions: none)\n",
      "ERROR: No matching distribution found for pwd\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J7xOqxRKVif"
   },
   "source": [
    "\n",
    "## Cross Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUqgZaJYhJbX"
   },
   "source": [
    "A cross encoder is a bert-based model that predicts a relevance score for a pair of sentences (late-interaction)\n",
    "\n",
    "* We have different models to choose from the list below.\n",
    "* Set the number of models to train, we train 10 models to get average performance.\n",
    "\n",
    "* choose the experiment mode\n",
    "    1.  QQA23_TaskA_qrcd_v1.2  ➡ normal training with official training data and validation with official validation data.  \n",
    "    2.  QQA23_TaskA_qrcd_v1.2_merged ➡ combining training and validation for training and perform inference using hidden split (done for testing phase).\n",
    "    3. tafseer  ➡ For tafseer pratraining data pairs\n",
    "    4. pre-train ➡ For tydi-qa pratraining data pairs\n",
    "\n",
    "[Check this for more details](https://www.sbert.net/examples/applications/cross-encoder/README.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PsNLwCpQssir"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medob\\Desktop\\Graduation Project\\Quran\\Quran QA 2023\\Task-A\\cross_encoder\\trainer.py\", line 6, in <module>\n",
      "    from sentence_transformers import InputExample, CrossEncoder\n",
      "  File \"c:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\__init__.py\", line 10, in <module>\n",
      "    from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder\n",
      "  File \"c:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py\", line 3, in <module>\n",
      "    from .CrossEncoder import CrossEncoder\n",
      "  File \"c:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py\", line 18, in <module>\n",
      "    from sentence_transformers.evaluation.SentenceEvaluator import SentenceEvaluator\n",
      "  File \"c:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\evaluation\\__init__.py\", line 3, in <module>\n",
      "    from .BinaryClassificationEvaluator import BinaryClassificationEvaluator\n",
      "  File \"c:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\evaluation\\BinaryClassificationEvaluator.py\", line 10, in <module>\n",
      "    from sklearn.metrics import average_precision_score\n",
      "  File \"c:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\__init__.py\", line 82, in <module>\n",
      "    from .base import clone\n",
      "  File \"c:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 17, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"c:\\Users\\medob\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 24, in <module>\n",
      "    from .murmurhash import murmurhash3_32\n",
      "  File \"sklearn\\utils\\murmurhash.pyx\", line 1, in init sklearn.utils.murmurhash\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import choice\n",
    "import glob\n",
    "#deepset/roberta-base-squad2 , deepset/bert-medium-squad2-distilled\n",
    "#Running : ahotrod/electra_large_discriminator_squad2_512\n",
    "# next : deepset/electra-base-squad2\n",
    "model_name = r\"ahotrod/electra_large_discriminator_squad2_512\"\n",
    "num_models = 1 # @param {type:\"integer\"}\n",
    "\n",
    "experiment_mode = \"QQA23_TaskA_qrcd_v1.2\"  # @param [\"QQA23_TaskA_qrcd_v1.2\", \"QQA23_TaskA_qrcd_v1.2_merged\",\"all_dev\",\"pre-train\",\"tafseer\"]\n",
    "\n",
    "lr = \"1e-6\"  # @param [\"2e-5\",\"1e-5\",\"5e-6\",\"2e-6\",\"1e-6\"]\n",
    "\n",
    "\n",
    "for idx in range(num_models):\n",
    "    out_file = f\"{idx}-out.txt\"\n",
    "    err_file = f\"{idx}-err.txt\"\n",
    "    doc_file=\"data/new_transeleted_quran.tsv\"\n",
    "\n",
    "    if experiment_mode == \"QQA23_TaskA_qrcd_v1.2_merged\":\n",
    "        train_qrel_file = \"data/QQA23_TaskA_qrels_merged.gold\"\n",
    "        train_query_file = \"data/QQA23_TaskA_merged.tsv\"\n",
    "    elif experiment_mode == \"QQA23_TaskA_qrcd_v1.2\":\n",
    "        # train_qrel_file = \"data/praphrased_train.gold\"\n",
    "        train_query_file = \"data/Transelted_train.tsv\"\n",
    "        train_qrel_file = \"data/QQA23_TaskA_qrels_train.gold\"\n",
    "        # train_query_file = \"data/QQA23_TaskA_train.tsv\"\n",
    "    elif experiment_mode == \"all_dev\":\n",
    "        print(\"mode3\")\n",
    "        train_qrel_file = \"data/QQA23_TaskA_qrels_dev.gold\"\n",
    "        train_query_file = \"data/QQA23_TaskA_dev.tsv\"\n",
    "\n",
    "    validation_qrel_file = \"data/QQA23_TaskA_qrels_dev.gold\"\n",
    "    validation_query_file = \"data/Transeleted_dev.tsv\"\n",
    "\n",
    "  \n",
    "    test_qrel_file = \"data/QQA23_TaskA_qrels_test.gold\"\n",
    "    test_query_file = \"data/Transeleted_test.tsv\"\n",
    "    num_train_epochs = 5\n",
    "    pre_train = False\n",
    "    do_predict = True\n",
    "    do_eval= True\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    if experiment_mode == \"pre-train\":\n",
    "        doc_file=\"data/TYDI_QA_DOC.tsv\"\n",
    "        train_qrel_file = \"data/TYDI_QA_qrels_train.gold\"\n",
    "        train_query_file = \"data/TYDI_QA_train.tsv\"\n",
    "        validation_qrel_file = \"data/TYDI_QA_qrels_dev.gold\"\n",
    "        validation_query_file = \"data/TYDI_QA_dev.tsv\"\n",
    "        test_qrel_file = None\n",
    "        test_query_file = None\n",
    "        pre_train = True\n",
    "        do_predict = False\n",
    "        num_train_epochs = 2\n",
    "\n",
    "    if experiment_mode == \"tafseer\":\n",
    "        doc_file=\"data/tafseer_docs.tsv\"\n",
    "        train_qrel_file = \"data/tafseer-qrel.tsv\"\n",
    "        train_query_file = \"data/tafseer-query.tsv\"\n",
    "        validation_qrel_file = None\n",
    "        validation_query_file = None\n",
    "        test_qrel_file = None\n",
    "        test_query_file = None\n",
    "        pre_train = False\n",
    "        do_eval= False\n",
    "        do_predict = False\n",
    "        num_train_epochs = 5\n",
    "\n",
    "\n",
    "    output_folder = os.path.split(model_name)[-1] + f\"-cross-v1.3-fine-tuned-{float(lr)}\"\n",
    "\n",
    "    batch_size = 4 \n",
    "\n",
    "\n",
    "    !git pull\n",
    "    !rm -r $output_folder\n",
    "\n",
    "    !python \"cross_encoder/trainer.py\" \\\n",
    "        --model_name_or_path  \"sultan/BioM-ELECTRA-Base-SQuAD2-BioASQ8B\" \\\n",
    "        --do_train True \\\n",
    "        --do_eval $do_eval \\\n",
    "        --do_predict $do_predict \\\n",
    "        --train_qrel_file $train_qrel_file \\\n",
    "        --train_query_file  $train_query_file \\\n",
    "        --validation_qrel_file  $validation_qrel_file \\\n",
    "        --validation_query_file $validation_query_file \\\n",
    "        --test_qrel_file $test_qrel_file  \\\n",
    "        --test_query_file  $test_query_file \\\n",
    "        --doc_file $doc_file \\\n",
    "        --learning_rate $lr \\\n",
    "        --num_train_epochs $num_train_epochs \\\n",
    "        --max_seq_length 256 \\\n",
    "        --output_dir \"$output_folder\" \\\n",
    "        --per_device_eval_batch_size 4 \\\n",
    "        --per_device_train_batch_size 4 \\\n",
    "        --save_steps 2 \\\n",
    "        --overwrite_output_dir \\\n",
    "        --fp16 \\\n",
    "        --gradient_accumulation_steps 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWCDuq2NwDEt"
   },
   "source": [
    "# Dual-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycwRw1e-hp6P"
   },
   "source": [
    "A dual-encoder is a bert-based model that predicts a relevance score for a pair of sentences represented individually (representational-based).\n",
    "The following cells trains ➡ infers ➡ mines hard negatives ➡ trains again.\n",
    "\n",
    "* We have different models to choose from the list below.\n",
    "* Set the number of models to train, we train 10 models to get average performance.\n",
    "\n",
    "* choose the experiment mode\n",
    "    1. QQA  ➡ normal training with official training data and validation with official validation data.  \n",
    "    2. QQA-merged ➡ combining training and validation for training and perform inference using hidden split (done for testing phase).\n",
    "    3. TYDI ➡ For tydi-qa pratraining data pairs\n",
    "\n",
    "[Check DRhard repo for more details](https://github.com/jingtaozhan/DRhard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (0.25.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (9.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\medob\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\medob\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "   ---------------------------------------- 0.0/255.8 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/255.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 122.9/255.8 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 255.8/255.8 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXI1IZUJ-SKG"
   },
   "source": [
    "# Analysis and ensemble\n",
    "\n",
    "**Once the training is made you will find a dump file saved!**\n",
    "\n",
    "**Once the training is made you will find a dump file saved!**\n",
    "\n",
    "something like: araelectra-base-discriminator-tafseer-pairs-fine-tuned-1e-06-5254-train.zip\n",
    "This is an araelectra-base-discriminator-tafseer-pairs fine-tuned model with:\n",
    "1. learning rate of 1e-06.\n",
    "2. A random starting seed of 5254.\n",
    "4. train.zip means training data is used\n",
    "\n",
    "This dump file contains models prediction for the given eval or test data.\n",
    "\n",
    "You can look at the **analysis** directory of the repo for more details.\n",
    "You can group dump files into folders:\n",
    "1. run **performance_analysis.py** script to process and get results for single models and ensemble models\n",
    "   - **retrieval_ensemble.py** is consumed by **performance_analysis.py** to implement the ensemble logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/quran-qa/Quran QA 2023/Task-A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'quran-qa/Quran QA 2023/Task-A'\n",
      "/teamspace/studios/this_studio/quran-qa/Quran QA 2023/Task-A\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%cd quran-qa/Quran\\ QA\\ 2023/Task-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wwXLK2Kx-Rv2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "artifacts/dumps 0.7648741075698899\n"
     ]
    }
   ],
   "source": [
    "!python analysis/performance_analysis.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3J7xOqxRKVif",
    "iWCDuq2NwDEt"
   ],
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
